{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0088332",
   "metadata": {},
   "source": [
    "# Classification of Highly Valued Stats and Metrics Between Drafted and Non-Drafted NCAA Basketball Players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9d84f",
   "metadata": {},
   "source": [
    "### Liam Shen, Vidyut Ramanan, Jasmine Duong, John Mehlenbacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3389302",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da990b50",
   "metadata": {},
   "source": [
    "The NBA draft is often an unknown gamble, as players drafted high sometimes fail, and players drafted low sometimes succeed. Hundreds, if not thousands, of college basketball years declare for the draft, with only 60 players hearing their name called on draft night. Each player is different, with different playstyles, and with varying amounts of success within college. This project aims to categorize college success and identify the stats or metrics that are most important for a college basketball player with dreams of  getting drafted into the NBA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dbc266",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b9915",
   "metadata": {},
   "source": [
    "The NBA draft is often a shot in the dark based on the stats and  metrics of college basketball players that stand out above others. This may be points scored per game, or assists given out pre game, but many players often specialize in one category of stats/metrics, each with different ways of playing the game of basketball.\n",
    "\n",
    "### Problem\n",
    "However, there have been notable college superstars that were drafted very high, who became failures in the NBA, while there have also been undrafted players, with very few stats that stood out, that have become fantastic basketball players.  The most basic of stats that are measured are often pts, ast, and treb, which signify the points, assists, and rebound per game[3]. Furthermore, players may have different roles and playstyles within a team, so one stat must  be valued above another. \n",
    "\n",
    "The issue with these stats is that it doesn’t take into account the effect the player has relative to the game. Underlying metrics such as PER, and BPM may have a bigger impact on a player being drafted or not, when compared to aforementioned stats.[5] And despite this, player’s each year with outstanding metrics still fail in the NBA, leading us to the problem: what stats or metrics are the most important for college players wanting to be drafted?\n",
    "\n",
    "### Motivation & Objectives\n",
    "While we had varying degrees of knowledge of each sport, we all agreed that sports can be a gold mine for data science. Sports are great for data because every single second is measured and recorded for every single player in every game.  Our goal is accurately categorize the stats and metrics that have the most value in the probability of getting drafted. With such extensive data resources readily available, we have the tools to analyze college statistics for individual players to find the strongest measure of performance. \n",
    "\n",
    "Our objective is to clean the NCAA dataset to accurately identify trends.. We want further level the playing field to college athletes looking to take their career to the next level that may not have conventionally high statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a0b71",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919d176",
   "metadata": {},
   "source": [
    "Our dataset was obtained from Kaggle in the form of a csv file. This data was originally collected on Basketball Reference and organized by a Kaggle user, and we decided to further clean and analyze that dataset since it was a large dataset containing all college NCAA basketball players and their stats from 2009 – 2021.\n",
    "\n",
    "First we must load in and examine our data. We do so using the desribe method, head method, and info method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbbf551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8109af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/dgm8krdj5bd42dmfqxcjj7200000gp/T/ipykernel_34610/802242054.py:2: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"2009_2021Basketball.csv\")\n"
     ]
    }
   ],
   "source": [
    "#loading in the csv \n",
    "data = pd.read_csv(\"2009_2021Basketball.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa52a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61061 entries, 0 to 61060\n",
      "Data columns (total 66 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   player_name                      61061 non-null  object \n",
      " 1   team                             61061 non-null  object \n",
      " 2   conf                             61061 non-null  object \n",
      " 3   GP                               61061 non-null  int64  \n",
      " 4   Min_per                          61061 non-null  float64\n",
      " 5   Ortg                             61061 non-null  float64\n",
      " 6   usg                              61061 non-null  float64\n",
      " 7   eFG                              61061 non-null  float64\n",
      " 8   TS_per                           61061 non-null  float64\n",
      " 9   ORB_per                          61061 non-null  float64\n",
      " 10  DRB_per                          61061 non-null  float64\n",
      " 11  AST_per                          61061 non-null  float64\n",
      " 12  TO_per                           61061 non-null  float64\n",
      " 13  FTM                              61061 non-null  int64  \n",
      " 14  FTA                              61061 non-null  int64  \n",
      " 15  FT_per                           61061 non-null  float64\n",
      " 16  twoPM                            61061 non-null  int64  \n",
      " 17  twoPA                            61061 non-null  int64  \n",
      " 18  twoP_per                         61061 non-null  float64\n",
      " 19  TPM                              61061 non-null  int64  \n",
      " 20  TPA                              61061 non-null  int64  \n",
      " 21  TP_per                           61061 non-null  float64\n",
      " 22  blk_per                          61061 non-null  float64\n",
      " 23  stl_per                          61061 non-null  float64\n",
      " 24  ftr                              61061 non-null  float64\n",
      " 25  yr                               60787 non-null  object \n",
      " 26  ht                               60975 non-null  object \n",
      " 27  num                              56304 non-null  object \n",
      " 28  porpag                           61061 non-null  float64\n",
      " 29  adjoe                            61061 non-null  float64\n",
      " 30  pfr                              61061 non-null  float64\n",
      " 31  year                             61061 non-null  int64  \n",
      " 32  pid                              61061 non-null  int64  \n",
      " 33  type                             61061 non-null  object \n",
      " 34  Rec Rank                         18470 non-null  float64\n",
      " 35  ast/tov                          56334 non-null  float64\n",
      " 36  rimmade                          54732 non-null  float64\n",
      " 37  rimmade+rimmiss                  54732 non-null  float64\n",
      " 38  midmade                          54732 non-null  float64\n",
      " 39  midmade+midmiss                  54732 non-null  float64\n",
      " 40  rimmade/(rimmade+rimmiss)        50951 non-null  float64\n",
      " 41  midmade/(midmade+midmiss)        50676 non-null  float64\n",
      " 42  dunksmade                        54732 non-null  float64\n",
      " 43  dunksmiss+dunksmade              54732 non-null  float64\n",
      " 44  dunksmade/(dunksmade+dunksmiss)  27551 non-null  float64\n",
      " 45  pick                             1435 non-null   float64\n",
      " 46  drtg                             61016 non-null  float64\n",
      " 47  adrtg                            61016 non-null  float64\n",
      " 48  dporpag                          61016 non-null  float64\n",
      " 49  stops                            61016 non-null  float64\n",
      " 50  bpm                              61016 non-null  float64\n",
      " 51  obpm                             61016 non-null  float64\n",
      " 52  dbpm                             61016 non-null  float64\n",
      " 53  gbpm                             61016 non-null  float64\n",
      " 54  mp                               61023 non-null  float64\n",
      " 55  ogbpm                            61016 non-null  float64\n",
      " 56  dgbpm                            61016 non-null  float64\n",
      " 57  oreb                             61023 non-null  float64\n",
      " 58  dreb                             61023 non-null  float64\n",
      " 59  treb                             61023 non-null  float64\n",
      " 60  ast                              61023 non-null  float64\n",
      " 61  stl                              61023 non-null  float64\n",
      " 62  blk                              61023 non-null  float64\n",
      " 63  pts                              61023 non-null  float64\n",
      " 64  Unnamed: 64                      56377 non-null  object \n",
      " 65  Unnamed: 65                      61016 non-null  float64\n",
      "dtypes: float64(49), int64(9), object(8)\n",
      "memory usage: 30.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#examine the data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9de78d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GP</th>\n",
       "      <th>Min_per</th>\n",
       "      <th>Ortg</th>\n",
       "      <th>usg</th>\n",
       "      <th>eFG</th>\n",
       "      <th>TS_per</th>\n",
       "      <th>ORB_per</th>\n",
       "      <th>DRB_per</th>\n",
       "      <th>AST_per</th>\n",
       "      <th>TO_per</th>\n",
       "      <th>...</th>\n",
       "      <th>ogbpm</th>\n",
       "      <th>dgbpm</th>\n",
       "      <th>oreb</th>\n",
       "      <th>dreb</th>\n",
       "      <th>treb</th>\n",
       "      <th>ast</th>\n",
       "      <th>stl</th>\n",
       "      <th>blk</th>\n",
       "      <th>pts</th>\n",
       "      <th>Unnamed: 65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>61061.000000</td>\n",
       "      <td>61061.00000</td>\n",
       "      <td>61061.000000</td>\n",
       "      <td>61061.000000</td>\n",
       "      <td>61061.000000</td>\n",
       "      <td>61061.000000</td>\n",
       "      <td>61061.00000</td>\n",
       "      <td>61061.000000</td>\n",
       "      <td>61061.000000</td>\n",
       "      <td>61061.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>61016.000000</td>\n",
       "      <td>61016.000000</td>\n",
       "      <td>61023.000000</td>\n",
       "      <td>61023.000000</td>\n",
       "      <td>61023.000000</td>\n",
       "      <td>61023.000000</td>\n",
       "      <td>61023.000000</td>\n",
       "      <td>61023.000000</td>\n",
       "      <td>61023.000000</td>\n",
       "      <td>61016.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22.797760</td>\n",
       "      <td>37.12839</td>\n",
       "      <td>91.666396</td>\n",
       "      <td>18.126341</td>\n",
       "      <td>44.500768</td>\n",
       "      <td>47.584212</td>\n",
       "      <td>5.54225</td>\n",
       "      <td>12.704242</td>\n",
       "      <td>10.808699</td>\n",
       "      <td>20.225856</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.088743</td>\n",
       "      <td>-0.449119</td>\n",
       "      <td>0.764618</td>\n",
       "      <td>1.897561</td>\n",
       "      <td>2.662179</td>\n",
       "      <td>1.072777</td>\n",
       "      <td>0.529446</td>\n",
       "      <td>0.280996</td>\n",
       "      <td>5.773579</td>\n",
       "      <td>5.384933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.166805</td>\n",
       "      <td>28.05805</td>\n",
       "      <td>30.963736</td>\n",
       "      <td>6.253742</td>\n",
       "      <td>18.431761</td>\n",
       "      <td>17.640613</td>\n",
       "      <td>9.30561</td>\n",
       "      <td>10.755123</td>\n",
       "      <td>9.362704</td>\n",
       "      <td>12.318765</td>\n",
       "      <td>...</td>\n",
       "      <td>6.237674</td>\n",
       "      <td>3.336297</td>\n",
       "      <td>0.734824</td>\n",
       "      <td>1.483689</td>\n",
       "      <td>2.097767</td>\n",
       "      <td>1.169677</td>\n",
       "      <td>0.469614</td>\n",
       "      <td>0.414885</td>\n",
       "      <td>4.947872</td>\n",
       "      <td>5.581653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-141.539000</td>\n",
       "      <td>-100.984000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>9.30000</td>\n",
       "      <td>83.900000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>43.800000</td>\n",
       "      <td>1.80000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.426910</td>\n",
       "      <td>-1.737988</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.703200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.571400</td>\n",
       "      <td>0.326832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>35.60000</td>\n",
       "      <td>97.100000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>47.700000</td>\n",
       "      <td>50.850000</td>\n",
       "      <td>4.30000</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.578180</td>\n",
       "      <td>-0.320310</td>\n",
       "      <td>0.538500</td>\n",
       "      <td>1.636400</td>\n",
       "      <td>2.260900</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>4.483900</td>\n",
       "      <td>4.609915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>62.00000</td>\n",
       "      <td>106.900000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>53.300000</td>\n",
       "      <td>56.110000</td>\n",
       "      <td>8.10000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999816</td>\n",
       "      <td>1.064795</td>\n",
       "      <td>1.103400</td>\n",
       "      <td>2.771400</td>\n",
       "      <td>3.828600</td>\n",
       "      <td>1.533300</td>\n",
       "      <td>0.794100</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>9.064500</td>\n",
       "      <td>8.500773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>98.00000</td>\n",
       "      <td>834.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1576.60000</td>\n",
       "      <td>1385.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>225.485000</td>\n",
       "      <td>78.985500</td>\n",
       "      <td>5.933300</td>\n",
       "      <td>11.545500</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.258100</td>\n",
       "      <td>30.090900</td>\n",
       "      <td>127.274000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 GP      Min_per          Ortg           usg           eFG  \\\n",
       "count  61061.000000  61061.00000  61061.000000  61061.000000  61061.000000   \n",
       "mean      22.797760     37.12839     91.666396     18.126341     44.500768   \n",
       "std       10.166805     28.05805     30.963736      6.253742     18.431761   \n",
       "min        1.000000      0.00000      0.000000      0.000000      0.000000   \n",
       "25%       15.000000      9.30000     83.900000     14.500000     40.000000   \n",
       "50%       27.000000     35.60000     97.100000     18.100000     47.700000   \n",
       "75%       31.000000     62.00000    106.900000     21.800000     53.300000   \n",
       "max       41.000000     98.00000    834.000000     50.000000    150.000000   \n",
       "\n",
       "             TS_per      ORB_per       DRB_per       AST_per        TO_per  \\\n",
       "count  61061.000000  61061.00000  61061.000000  61061.000000  61061.000000   \n",
       "mean      47.584212      5.54225     12.704242     10.808699     20.225856   \n",
       "std       17.640613      9.30561     10.755123      9.362704     12.318765   \n",
       "min        0.000000      0.00000      0.000000      0.000000      0.000000   \n",
       "25%       43.800000      1.80000      8.400000      4.600000     14.700000   \n",
       "50%       50.850000      4.30000     11.900000      9.000000     19.100000   \n",
       "75%       56.110000      8.10000     16.100000     15.000000     24.400000   \n",
       "max      150.000000   1576.60000   1385.000000    100.000000    100.000000   \n",
       "\n",
       "       ...         ogbpm         dgbpm          oreb          dreb  \\\n",
       "count  ...  61016.000000  61016.000000  61023.000000  61023.000000   \n",
       "mean   ...     -2.088743     -0.449119      0.764618      1.897561   \n",
       "std    ...      6.237674      3.336297      0.734824      1.483689   \n",
       "min    ...   -141.539000   -100.984000      0.000000      0.000000   \n",
       "25%    ...     -4.426910     -1.737988      0.222200      0.703200   \n",
       "50%    ...     -1.578180     -0.320310      0.538500      1.636400   \n",
       "75%    ...      0.999816      1.064795      1.103400      2.771400   \n",
       "max    ...    225.485000     78.985500      5.933300     11.545500   \n",
       "\n",
       "               treb           ast           stl           blk           pts  \\\n",
       "count  61023.000000  61023.000000  61023.000000  61023.000000  61023.000000   \n",
       "mean       2.662179      1.072777      0.529446      0.280996      5.773579   \n",
       "std        2.097767      1.169677      0.469614      0.414885      4.947872   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        1.000000      0.222200      0.157900      0.000000      1.571400   \n",
       "50%        2.260900      0.677400      0.424200      0.133300      4.483900   \n",
       "75%        3.828600      1.533300      0.794100      0.363600      9.064500   \n",
       "max       15.000000     10.000000      4.000000      5.258100     30.090900   \n",
       "\n",
       "        Unnamed: 65  \n",
       "count  61016.000000  \n",
       "mean       5.384933  \n",
       "std        5.581653  \n",
       "min        0.000000  \n",
       "25%        0.326832  \n",
       "50%        4.609915  \n",
       "75%        8.500773  \n",
       "max      127.274000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c157d490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count               61061\n",
       "unique              25719\n",
       "top       Austin Williams\n",
       "freq                   15\n",
       "Name: player_name, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at duplicate players \n",
    "data['player_name'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f8dc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_name</th>\n",
       "      <th>year</th>\n",
       "      <th>pick</th>\n",
       "      <th>team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30968</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>North Carolina A&amp;T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30981</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drexel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34403</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>North Carolina A&amp;T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34414</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drexel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38233</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>North Carolina A&amp;T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38241</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drexel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40374</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42317</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>North Carolina A&amp;T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42322</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drexel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43850</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45258</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47672</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48621</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51690</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56462</th>\n",
       "      <td>Austin Williams</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hartford</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           player_name  year  pick                team\n",
       "30968  Austin Williams  2015   NaN  North Carolina A&T\n",
       "30981  Austin Williams  2015   NaN              Drexel\n",
       "34403  Austin Williams  2016   NaN  North Carolina A&T\n",
       "34414  Austin Williams  2016   NaN              Drexel\n",
       "38233  Austin Williams  2017   NaN  North Carolina A&T\n",
       "38241  Austin Williams  2017   NaN              Drexel\n",
       "40374  Austin Williams  2017   NaN                Yale\n",
       "42317  Austin Williams  2018   NaN  North Carolina A&T\n",
       "42322  Austin Williams  2018   NaN              Drexel\n",
       "43850  Austin Williams  2018   NaN                Yale\n",
       "45258  Austin Williams  2018   NaN              Marist\n",
       "47672  Austin Williams  2019   NaN                Yale\n",
       "48621  Austin Williams  2019   NaN              Marist\n",
       "51690  Austin Williams  2020   NaN                Yale\n",
       "56462  Austin Williams  2021   NaN            Hartford"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of duplicate player\n",
    "data[data['player_name'] == 'Austin Williams'][['player_name', 'year','pick','team']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ee693b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_name</th>\n",
       "      <th>team</th>\n",
       "      <th>conf</th>\n",
       "      <th>GP</th>\n",
       "      <th>Min_per</th>\n",
       "      <th>Ortg</th>\n",
       "      <th>usg</th>\n",
       "      <th>eFG</th>\n",
       "      <th>TS_per</th>\n",
       "      <th>ORB_per</th>\n",
       "      <th>...</th>\n",
       "      <th>dgbpm</th>\n",
       "      <th>oreb</th>\n",
       "      <th>dreb</th>\n",
       "      <th>treb</th>\n",
       "      <th>ast</th>\n",
       "      <th>stl</th>\n",
       "      <th>blk</th>\n",
       "      <th>pts</th>\n",
       "      <th>Unnamed: 64</th>\n",
       "      <th>Unnamed: 65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeAndrae Ross</td>\n",
       "      <td>South Alabama</td>\n",
       "      <td>SB</td>\n",
       "      <td>26</td>\n",
       "      <td>29.5</td>\n",
       "      <td>97.3</td>\n",
       "      <td>16.6</td>\n",
       "      <td>42.5</td>\n",
       "      <td>44.43</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.941150</td>\n",
       "      <td>0.1923</td>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8077</td>\n",
       "      <td>1.1923</td>\n",
       "      <td>0.3462</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>3.8846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.22026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pooh Williams</td>\n",
       "      <td>Utah St.</td>\n",
       "      <td>WAC</td>\n",
       "      <td>34</td>\n",
       "      <td>60.9</td>\n",
       "      <td>108.3</td>\n",
       "      <td>14.9</td>\n",
       "      <td>52.4</td>\n",
       "      <td>54.48</td>\n",
       "      <td>3.8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247934</td>\n",
       "      <td>0.6765</td>\n",
       "      <td>1.2647</td>\n",
       "      <td>1.9412</td>\n",
       "      <td>1.8235</td>\n",
       "      <td>0.4118</td>\n",
       "      <td>0.2353</td>\n",
       "      <td>5.9412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.94375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jesus Verdejo</td>\n",
       "      <td>South Florida</td>\n",
       "      <td>BE</td>\n",
       "      <td>27</td>\n",
       "      <td>72.0</td>\n",
       "      <td>96.2</td>\n",
       "      <td>21.8</td>\n",
       "      <td>45.7</td>\n",
       "      <td>47.98</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.883163</td>\n",
       "      <td>0.6296</td>\n",
       "      <td>2.3333</td>\n",
       "      <td>2.9630</td>\n",
       "      <td>1.9630</td>\n",
       "      <td>0.4815</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12.1852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.92680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mike Hornbuckle</td>\n",
       "      <td>Pepperdine</td>\n",
       "      <td>WCC</td>\n",
       "      <td>30</td>\n",
       "      <td>44.5</td>\n",
       "      <td>97.7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>53.69</td>\n",
       "      <td>4.1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393459</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>1.4333</td>\n",
       "      <td>2.1333</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.1333</td>\n",
       "      <td>4.9333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.77427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anthony Brown</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>BW</td>\n",
       "      <td>33</td>\n",
       "      <td>56.2</td>\n",
       "      <td>96.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>52.8</td>\n",
       "      <td>54.31</td>\n",
       "      <td>8.3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.668318</td>\n",
       "      <td>1.4242</td>\n",
       "      <td>3.3030</td>\n",
       "      <td>4.7273</td>\n",
       "      <td>0.8485</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>7.5758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nick Rodgers</td>\n",
       "      <td>Butler</td>\n",
       "      <td>Horz</td>\n",
       "      <td>6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.744910</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dana Smith</td>\n",
       "      <td>Longwood</td>\n",
       "      <td>ind</td>\n",
       "      <td>27</td>\n",
       "      <td>77.8</td>\n",
       "      <td>104.8</td>\n",
       "      <td>23.0</td>\n",
       "      <td>53.4</td>\n",
       "      <td>56.30</td>\n",
       "      <td>6.8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.908391</td>\n",
       "      <td>2.1481</td>\n",
       "      <td>3.8889</td>\n",
       "      <td>6.0370</td>\n",
       "      <td>1.8148</td>\n",
       "      <td>1.7778</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>15.2593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.22107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Matt Beck</td>\n",
       "      <td>Fordham</td>\n",
       "      <td>A10</td>\n",
       "      <td>19</td>\n",
       "      <td>10.4</td>\n",
       "      <td>131.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.086900</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.3158</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.50384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Justin Drummond</td>\n",
       "      <td>Wagner</td>\n",
       "      <td>NEC</td>\n",
       "      <td>30</td>\n",
       "      <td>82.8</td>\n",
       "      <td>99.7</td>\n",
       "      <td>20.5</td>\n",
       "      <td>48.8</td>\n",
       "      <td>53.07</td>\n",
       "      <td>2.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919272</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>3.9333</td>\n",
       "      <td>4.7333</td>\n",
       "      <td>4.1333</td>\n",
       "      <td>1.7333</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.42016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jamal Smith</td>\n",
       "      <td>Wagner</td>\n",
       "      <td>NEC</td>\n",
       "      <td>30</td>\n",
       "      <td>80.4</td>\n",
       "      <td>92.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>43.5</td>\n",
       "      <td>45.29</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.913070</td>\n",
       "      <td>1.7333</td>\n",
       "      <td>3.4667</td>\n",
       "      <td>5.2000</td>\n",
       "      <td>1.7333</td>\n",
       "      <td>1.2333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>11.8667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.34007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tyrell Biggs</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>BE</td>\n",
       "      <td>34</td>\n",
       "      <td>57.9</td>\n",
       "      <td>113.9</td>\n",
       "      <td>13.7</td>\n",
       "      <td>55.3</td>\n",
       "      <td>56.37</td>\n",
       "      <td>9.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.557650</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>2.2941</td>\n",
       "      <td>4.2941</td>\n",
       "      <td>0.7059</td>\n",
       "      <td>0.4118</td>\n",
       "      <td>0.4706</td>\n",
       "      <td>6.4706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.14112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alex Hornat</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>BE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.609010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Simon Harris</td>\n",
       "      <td>North Carolina St.</td>\n",
       "      <td>ACC</td>\n",
       "      <td>19</td>\n",
       "      <td>18.4</td>\n",
       "      <td>95.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>45.2</td>\n",
       "      <td>48.39</td>\n",
       "      <td>6.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388021</td>\n",
       "      <td>0.6316</td>\n",
       "      <td>1.3684</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.2632</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>1.2632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.34027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Gary Wilkinson</td>\n",
       "      <td>Utah St.</td>\n",
       "      <td>WAC</td>\n",
       "      <td>34</td>\n",
       "      <td>79.9</td>\n",
       "      <td>124.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>57.9</td>\n",
       "      <td>63.97</td>\n",
       "      <td>7.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233409</td>\n",
       "      <td>1.6765</td>\n",
       "      <td>4.9706</td>\n",
       "      <td>6.6471</td>\n",
       "      <td>1.2353</td>\n",
       "      <td>0.4706</td>\n",
       "      <td>0.3824</td>\n",
       "      <td>16.8824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.11986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tawrence Walton</td>\n",
       "      <td>Chicago St.</td>\n",
       "      <td>ind</td>\n",
       "      <td>30</td>\n",
       "      <td>67.3</td>\n",
       "      <td>102.1</td>\n",
       "      <td>15.1</td>\n",
       "      <td>49.7</td>\n",
       "      <td>54.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.315280</td>\n",
       "      <td>2.5000</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1.2333</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>7.4333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.45590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Garrett Temple</td>\n",
       "      <td>LSU</td>\n",
       "      <td>SEC</td>\n",
       "      <td>35</td>\n",
       "      <td>75.2</td>\n",
       "      <td>104.4</td>\n",
       "      <td>15.9</td>\n",
       "      <td>43.1</td>\n",
       "      <td>49.61</td>\n",
       "      <td>4.8</td>\n",
       "      <td>...</td>\n",
       "      <td>3.121290</td>\n",
       "      <td>1.3143</td>\n",
       "      <td>3.2286</td>\n",
       "      <td>4.5429</td>\n",
       "      <td>3.8286</td>\n",
       "      <td>1.7143</td>\n",
       "      <td>0.6857</td>\n",
       "      <td>7.0571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.19900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Carlos Strong</td>\n",
       "      <td>Boston University</td>\n",
       "      <td>AE</td>\n",
       "      <td>12</td>\n",
       "      <td>16.6</td>\n",
       "      <td>113.6</td>\n",
       "      <td>19.9</td>\n",
       "      <td>57.8</td>\n",
       "      <td>59.12</td>\n",
       "      <td>10.8</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.289520</td>\n",
       "      <td>1.5833</td>\n",
       "      <td>1.5833</td>\n",
       "      <td>3.1667</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>6.4167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.84619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sherrod Smith</td>\n",
       "      <td>Boston University</td>\n",
       "      <td>AE</td>\n",
       "      <td>10</td>\n",
       "      <td>2.8</td>\n",
       "      <td>116.9</td>\n",
       "      <td>10.3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>65.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.595500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Eulis Stephens</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>Horz</td>\n",
       "      <td>26</td>\n",
       "      <td>55.5</td>\n",
       "      <td>91.2</td>\n",
       "      <td>20.2</td>\n",
       "      <td>43.4</td>\n",
       "      <td>47.74</td>\n",
       "      <td>2.4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126528</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>2.0769</td>\n",
       "      <td>2.5385</td>\n",
       "      <td>1.0385</td>\n",
       "      <td>0.7308</td>\n",
       "      <td>0.2692</td>\n",
       "      <td>8.0769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.44915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Scott Brittain</td>\n",
       "      <td>Boston University</td>\n",
       "      <td>AE</td>\n",
       "      <td>27</td>\n",
       "      <td>52.1</td>\n",
       "      <td>100.4</td>\n",
       "      <td>19.5</td>\n",
       "      <td>47.0</td>\n",
       "      <td>52.28</td>\n",
       "      <td>9.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069262</td>\n",
       "      <td>1.8889</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.8889</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.6296</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>7.4444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Brett Gifford</td>\n",
       "      <td>Albany</td>\n",
       "      <td>AE</td>\n",
       "      <td>31</td>\n",
       "      <td>45.7</td>\n",
       "      <td>92.1</td>\n",
       "      <td>9.3</td>\n",
       "      <td>53.8</td>\n",
       "      <td>55.91</td>\n",
       "      <td>7.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.260390</td>\n",
       "      <td>1.2258</td>\n",
       "      <td>2.7097</td>\n",
       "      <td>3.9355</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.7742</td>\n",
       "      <td>2.5161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Valdas Sirutis</td>\n",
       "      <td>Boston University</td>\n",
       "      <td>AE</td>\n",
       "      <td>19</td>\n",
       "      <td>8.6</td>\n",
       "      <td>96.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>62.5</td>\n",
       "      <td>54.38</td>\n",
       "      <td>5.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441445</td>\n",
       "      <td>0.2632</td>\n",
       "      <td>0.4737</td>\n",
       "      <td>0.7368</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.4737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.74750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Corey Lowe</td>\n",
       "      <td>Boston University</td>\n",
       "      <td>AE</td>\n",
       "      <td>29</td>\n",
       "      <td>84.8</td>\n",
       "      <td>103.6</td>\n",
       "      <td>27.5</td>\n",
       "      <td>53.5</td>\n",
       "      <td>54.68</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.102900</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>3.3793</td>\n",
       "      <td>4.0345</td>\n",
       "      <td>4.1379</td>\n",
       "      <td>1.2414</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>17.1724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.46890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Grant Leiendecker</td>\n",
       "      <td>Butler</td>\n",
       "      <td>Horz</td>\n",
       "      <td>25</td>\n",
       "      <td>12.4</td>\n",
       "      <td>84.5</td>\n",
       "      <td>19.5</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.919000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>1.6800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.98530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Patrick Bouli</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MAAC</td>\n",
       "      <td>21</td>\n",
       "      <td>36.7</td>\n",
       "      <td>75.4</td>\n",
       "      <td>10.2</td>\n",
       "      <td>40.4</td>\n",
       "      <td>44.05</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714270</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>2.0952</td>\n",
       "      <td>2.2381</td>\n",
       "      <td>1.4286</td>\n",
       "      <td>0.8095</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>2.3810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.36494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Antoine Pearson</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MAAC</td>\n",
       "      <td>30</td>\n",
       "      <td>59.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>20.8</td>\n",
       "      <td>47.4</td>\n",
       "      <td>49.83</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799601</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1.5333</td>\n",
       "      <td>1.7333</td>\n",
       "      <td>2.3333</td>\n",
       "      <td>1.2333</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>7.5333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.39547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Jamel Ferguson</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MAAC</td>\n",
       "      <td>21</td>\n",
       "      <td>21.8</td>\n",
       "      <td>86.4</td>\n",
       "      <td>19.1</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.88</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.614910</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>1.1905</td>\n",
       "      <td>1.7619</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4762</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>3.2381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.64488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chris Harris</td>\n",
       "      <td>Navy</td>\n",
       "      <td>Pat</td>\n",
       "      <td>30</td>\n",
       "      <td>85.0</td>\n",
       "      <td>104.9</td>\n",
       "      <td>21.7</td>\n",
       "      <td>47.6</td>\n",
       "      <td>53.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155789</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>2.0333</td>\n",
       "      <td>2.5333</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>15.4333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.50300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>T.J. Topercer</td>\n",
       "      <td>Navy</td>\n",
       "      <td>Pat</td>\n",
       "      <td>16</td>\n",
       "      <td>7.2</td>\n",
       "      <td>91.5</td>\n",
       "      <td>14.3</td>\n",
       "      <td>30.8</td>\n",
       "      <td>46.31</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692803</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.55593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Brandon Adams</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MAAC</td>\n",
       "      <td>29</td>\n",
       "      <td>33.9</td>\n",
       "      <td>101.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49.2</td>\n",
       "      <td>56.44</td>\n",
       "      <td>10.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.247500</td>\n",
       "      <td>1.4138</td>\n",
       "      <td>1.5517</td>\n",
       "      <td>2.9655</td>\n",
       "      <td>0.2759</td>\n",
       "      <td>0.5172</td>\n",
       "      <td>0.6207</td>\n",
       "      <td>3.4138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          player_name                team  conf  GP  Min_per   Ortg   usg  \\\n",
       "0       DeAndrae Ross       South Alabama    SB  26     29.5   97.3  16.6   \n",
       "1       Pooh Williams            Utah St.   WAC  34     60.9  108.3  14.9   \n",
       "2       Jesus Verdejo       South Florida    BE  27     72.0   96.2  21.8   \n",
       "3     Mike Hornbuckle          Pepperdine   WCC  30     44.5   97.7  16.0   \n",
       "4       Anthony Brown             Pacific    BW  33     56.2   96.5  22.0   \n",
       "5        Nick Rodgers              Butler  Horz   6      0.7    0.0   0.0   \n",
       "6          Dana Smith            Longwood   ind  27     77.8  104.8  23.0   \n",
       "7           Matt Beck             Fordham   A10  19     10.4  131.9   3.3   \n",
       "8     Justin Drummond              Wagner   NEC  30     82.8   99.7  20.5   \n",
       "9         Jamal Smith              Wagner   NEC  30     80.4   92.5  23.0   \n",
       "10       Tyrell Biggs          Pittsburgh    BE  34     57.9  113.9  13.7   \n",
       "11        Alex Hornat         Connecticut    BE   2      0.1    0.0  28.9   \n",
       "12       Simon Harris  North Carolina St.   ACC  19     18.4   95.1   8.2   \n",
       "13     Gary Wilkinson            Utah St.   WAC  34     79.9  124.8  24.0   \n",
       "14    Tawrence Walton         Chicago St.   ind  30     67.3  102.1  15.1   \n",
       "15     Garrett Temple                 LSU   SEC  35     75.2  104.4  15.9   \n",
       "16      Carlos Strong   Boston University    AE  12     16.6  113.6  19.9   \n",
       "17      Sherrod Smith   Boston University    AE  10      2.8  116.9  10.3   \n",
       "18     Eulis Stephens             Detroit  Horz  26     55.5   91.2  20.2   \n",
       "19     Scott Brittain   Boston University    AE  27     52.1  100.4  19.5   \n",
       "20      Brett Gifford              Albany    AE  31     45.7   92.1   9.3   \n",
       "21     Valdas Sirutis   Boston University    AE  19      8.6   96.9   6.7   \n",
       "22         Corey Lowe   Boston University    AE  29     84.8  103.6  27.5   \n",
       "23  Grant Leiendecker              Butler  Horz  25     12.4   84.5  19.5   \n",
       "24      Patrick Bouli           Manhattan  MAAC  21     36.7   75.4  10.2   \n",
       "25    Antoine Pearson           Manhattan  MAAC  30     59.3   90.3  20.8   \n",
       "26     Jamel Ferguson           Manhattan  MAAC  21     21.8   86.4  19.1   \n",
       "27       Chris Harris                Navy   Pat  30     85.0  104.9  21.7   \n",
       "28      T.J. Topercer                Navy   Pat  16      7.2   91.5  14.3   \n",
       "29      Brandon Adams           Manhattan  MAAC  29     33.9  101.9  15.0   \n",
       "\n",
       "     eFG  TS_per  ORB_per  ...     dgbpm    oreb    dreb    treb     ast  \\\n",
       "0   42.5   44.43      1.6  ... -1.941150  0.1923  0.6154  0.8077  1.1923   \n",
       "1   52.4   54.48      3.8  ... -0.247934  0.6765  1.2647  1.9412  1.8235   \n",
       "2   45.7   47.98      2.1  ... -0.883163  0.6296  2.3333  2.9630  1.9630   \n",
       "3   53.6   53.69      4.1  ... -0.393459  0.7000  1.4333  2.1333  1.1000   \n",
       "4   52.8   54.31      8.3  ... -0.668318  1.4242  3.3030  4.7273  0.8485   \n",
       "5    0.0    0.00      0.0  ...  3.744910  0.0000  0.3333  0.3333  0.3333   \n",
       "6   53.4   56.30      6.8  ... -0.908391  2.1481  3.8889  6.0370  1.8148   \n",
       "7   90.0   90.00      2.6  ... -2.086900  0.1579  0.1579  0.3158  0.1053   \n",
       "8   48.8   53.07      2.9  ...  0.919272  0.8000  3.9333  4.7333  4.1333   \n",
       "9   43.5   45.29      6.4  ... -1.913070  1.7333  3.4667  5.2000  1.7333   \n",
       "10  55.3   56.37      9.8  ...  1.557650  2.0000  2.2941  4.2941  0.7059   \n",
       "11   0.0    0.00      0.0  ... -6.609010  0.0000  0.0000  0.0000  0.0000   \n",
       "12  45.2   48.39      6.9  ...  0.388021  0.6316  1.3684  2.0000  0.2632   \n",
       "13  57.9   63.97      7.2  ... -0.233409  1.6765  4.9706  6.6471  1.2353   \n",
       "14  49.7   54.46      9.4  ... -2.315280  2.5000  3.5000  6.0000  1.2333   \n",
       "15  43.1   49.61      4.8  ...  3.121290  1.3143  3.2286  4.5429  3.8286   \n",
       "16  57.8   59.12     10.8  ... -1.289520  1.5833  1.5833  3.1667  0.8333   \n",
       "17  50.0   65.73      0.0  ...  2.595500  0.0000  0.4000  0.4000  0.1000   \n",
       "18  43.4   47.74      2.4  ... -0.126528  0.4615  2.0769  2.5385  1.0385   \n",
       "19  47.0   52.28      9.3  ...  0.069262  1.8889  3.0000  4.8889  0.7778   \n",
       "20  53.8   55.91      7.6  ...  1.260390  1.2258  2.7097  3.9355  0.2258   \n",
       "21  62.5   54.38      5.5  ...  0.441445  0.2632  0.4737  0.7368  0.1053   \n",
       "22  53.5   54.68      2.1  ... -1.102900  0.6552  3.3793  4.0345  4.1379   \n",
       "23  38.5   38.96      0.0  ... -1.919000  0.0000  0.2800  0.2800  0.4000   \n",
       "24  40.4   44.05      0.7  ...  1.714270  0.1429  2.0952  2.2381  1.4286   \n",
       "25  47.4   49.83      0.9  ...  0.799601  0.2000  1.5333  1.7333  2.3333   \n",
       "26  39.4   39.88      5.0  ...  1.614910  0.5714  1.1905  1.7619  1.0000   \n",
       "27  47.6   53.29      1.6  ... -0.155789  0.5000  2.0333  2.5333  1.9000   \n",
       "28  30.8   46.31     11.0  ... -0.692803  0.5625  0.2500  0.8125  0.1875   \n",
       "29  49.2   56.44     10.9  ...  1.247500  1.4138  1.5517  2.9655  0.2759   \n",
       "\n",
       "       stl     blk      pts  Unnamed: 64  Unnamed: 65  \n",
       "0   0.3462  0.0385   3.8846          NaN      6.22026  \n",
       "1   0.4118  0.2353   5.9412          NaN      3.94375  \n",
       "2   0.4815  0.0000  12.1852          NaN     10.92680  \n",
       "3   0.5667  0.1333   4.9333          NaN      6.77427  \n",
       "4   0.4545  0.3333   7.5758          NaN      0.00000  \n",
       "5   0.0000  0.0000   0.0000          NaN      0.00000  \n",
       "6   1.7778  0.7407  15.2593          NaN      3.22107  \n",
       "7   0.1053  0.0000   0.4737          NaN      2.50384  \n",
       "8   1.7333  0.8000  10.5000          NaN      3.42016  \n",
       "9   1.2333  0.1000  11.8667          NaN      3.34007  \n",
       "10  0.4118  0.4706   6.4706          NaN      3.14112  \n",
       "11  0.0000  0.0000   0.0000          NaN      0.00000  \n",
       "12  0.1579  0.1579   1.2632          NaN      1.34027  \n",
       "13  0.4706  0.3824  16.8824          NaN      1.11986  \n",
       "14  0.9333  0.5000   7.4333          NaN      1.45590  \n",
       "15  1.7143  0.6857   7.0571          NaN      5.19900  \n",
       "16  0.2500  0.2500   6.4167          NaN      7.84619  \n",
       "17  0.2000  0.0000   0.7000          NaN      0.00000  \n",
       "18  0.7308  0.2692   8.0769          NaN      5.44915  \n",
       "19  0.6296  0.9259   7.4444          NaN      0.00000  \n",
       "20  0.2258  0.7742   2.5161          NaN      0.00000  \n",
       "21  0.1579  0.1053   0.4737          NaN      1.74750  \n",
       "22  1.2414  0.1379  17.1724          NaN     13.46890  \n",
       "23  0.0400  0.0800   1.6800          NaN     18.98530  \n",
       "24  0.8095  0.0476   2.3810          NaN      5.36494  \n",
       "25  1.2333  0.2333   7.5333          NaN      5.39547  \n",
       "26  0.4762  0.3333   3.2381          NaN      5.64488  \n",
       "27  1.4000  0.0333  15.4333          NaN     12.50300  \n",
       "28  0.1250  0.0000   1.0000          NaN      2.55593  \n",
       "29  0.5172  0.6207   3.4138          NaN      0.00000  \n",
       "\n",
       "[30 rows x 66 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36ebbb",
   "metadata": {},
   "source": [
    "The data appears to be good for the most part but there are some things that we must change. First there are some columns that we don't need so because the stats are either redundant or unnecessary. We will drop those columns below. There are many columns with data that are the object data type so we will have to cast them to the right data type. There are also some null or missing values for players. Since these don't make up a substantial portion of the dataset, we will drop them. We also found that there are many players with duplicate names. After looking in to the data we realized that this was because there were players with the same name and also because a player's stats across multiple years could be in the dataframe and each year would take up a row so one player could have more than one row. To solve this issue, we will consider a player a dupicate if they have the same name, year, team, and pick. We keep only the latest year. We have also decided to encode the pick as 0 if they were not drafted or 1 if they were drafted. Right now pick represents the position overall that they were drafted and is null if they were not drafted. Furthermore, to test the importance of metrics compared to on-court stats, we will add two essential metrics for player performance. The PER, or Player Efficiency Rating, is calculated using a linear formula derived from the league averages, and players with no minutes were automatically set to 0 as that meant they did not play. The netRtg, or net rating, is calculating by subtracting drtg from Ortg, hence calculating a player’s impact on both ends of the court, per 100 possessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdced5a",
   "metadata": {},
   "source": [
    "##  Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "063ebed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_name</th>\n",
       "      <th>team</th>\n",
       "      <th>conf</th>\n",
       "      <th>GP</th>\n",
       "      <th>Ortg</th>\n",
       "      <th>usg</th>\n",
       "      <th>eFG</th>\n",
       "      <th>TS_per</th>\n",
       "      <th>FTM</th>\n",
       "      <th>FTA</th>\n",
       "      <th>...</th>\n",
       "      <th>mp</th>\n",
       "      <th>oreb</th>\n",
       "      <th>dreb</th>\n",
       "      <th>treb</th>\n",
       "      <th>ast</th>\n",
       "      <th>stl</th>\n",
       "      <th>blk</th>\n",
       "      <th>pts</th>\n",
       "      <th>PER</th>\n",
       "      <th>netRtg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeAndrae Ross</td>\n",
       "      <td>South Alabama</td>\n",
       "      <td>SB</td>\n",
       "      <td>26</td>\n",
       "      <td>97.3</td>\n",
       "      <td>16.6</td>\n",
       "      <td>42.5</td>\n",
       "      <td>44.43</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>14.5769</td>\n",
       "      <td>0.1923</td>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8077</td>\n",
       "      <td>1.1923</td>\n",
       "      <td>0.3462</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>3.8846</td>\n",
       "      <td>4.603199</td>\n",
       "      <td>-11.0210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pooh Williams</td>\n",
       "      <td>Utah St.</td>\n",
       "      <td>WAC</td>\n",
       "      <td>34</td>\n",
       "      <td>108.3</td>\n",
       "      <td>14.9</td>\n",
       "      <td>52.4</td>\n",
       "      <td>54.48</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>24.5294</td>\n",
       "      <td>0.6765</td>\n",
       "      <td>1.2647</td>\n",
       "      <td>1.9412</td>\n",
       "      <td>1.8235</td>\n",
       "      <td>0.4118</td>\n",
       "      <td>0.2353</td>\n",
       "      <td>5.9412</td>\n",
       "      <td>7.120085</td>\n",
       "      <td>3.2220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jesus Verdejo</td>\n",
       "      <td>South Florida</td>\n",
       "      <td>BE</td>\n",
       "      <td>27</td>\n",
       "      <td>96.2</td>\n",
       "      <td>21.8</td>\n",
       "      <td>45.7</td>\n",
       "      <td>47.98</td>\n",
       "      <td>45</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>33.1852</td>\n",
       "      <td>0.6296</td>\n",
       "      <td>2.3333</td>\n",
       "      <td>2.9630</td>\n",
       "      <td>1.9630</td>\n",
       "      <td>0.4815</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12.1852</td>\n",
       "      <td>8.264053</td>\n",
       "      <td>-11.3560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mike Hornbuckle</td>\n",
       "      <td>Pepperdine</td>\n",
       "      <td>WCC</td>\n",
       "      <td>30</td>\n",
       "      <td>97.7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>53.69</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>17.9667</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>1.4333</td>\n",
       "      <td>2.1333</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>0.5667</td>\n",
       "      <td>0.1333</td>\n",
       "      <td>4.9333</td>\n",
       "      <td>8.035690</td>\n",
       "      <td>-11.1580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anthony Brown</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>BW</td>\n",
       "      <td>33</td>\n",
       "      <td>96.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>52.8</td>\n",
       "      <td>54.31</td>\n",
       "      <td>64</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>22.9091</td>\n",
       "      <td>1.4242</td>\n",
       "      <td>3.3030</td>\n",
       "      <td>4.7273</td>\n",
       "      <td>0.8485</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>7.5758</td>\n",
       "      <td>9.470778</td>\n",
       "      <td>-4.5520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nick Rodgers</td>\n",
       "      <td>Butler</td>\n",
       "      <td>Horz</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-92.7329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dana Smith</td>\n",
       "      <td>Longwood</td>\n",
       "      <td>ind</td>\n",
       "      <td>27</td>\n",
       "      <td>104.8</td>\n",
       "      <td>23.0</td>\n",
       "      <td>53.4</td>\n",
       "      <td>56.30</td>\n",
       "      <td>94</td>\n",
       "      <td>143</td>\n",
       "      <td>...</td>\n",
       "      <td>31.1111</td>\n",
       "      <td>2.1481</td>\n",
       "      <td>3.8889</td>\n",
       "      <td>6.0370</td>\n",
       "      <td>1.8148</td>\n",
       "      <td>1.7778</td>\n",
       "      <td>0.7407</td>\n",
       "      <td>15.2593</td>\n",
       "      <td>17.610147</td>\n",
       "      <td>3.5910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Matt Beck</td>\n",
       "      <td>Fordham</td>\n",
       "      <td>A10</td>\n",
       "      <td>19</td>\n",
       "      <td>131.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1579</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.3158</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4737</td>\n",
       "      <td>0.930489</td>\n",
       "      <td>13.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Justin Drummond</td>\n",
       "      <td>Wagner</td>\n",
       "      <td>NEC</td>\n",
       "      <td>30</td>\n",
       "      <td>99.7</td>\n",
       "      <td>20.5</td>\n",
       "      <td>48.8</td>\n",
       "      <td>53.07</td>\n",
       "      <td>76</td>\n",
       "      <td>109</td>\n",
       "      <td>...</td>\n",
       "      <td>33.2667</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>3.9333</td>\n",
       "      <td>4.7333</td>\n",
       "      <td>4.1333</td>\n",
       "      <td>1.7333</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>13.165177</td>\n",
       "      <td>2.1018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jamal Smith</td>\n",
       "      <td>Wagner</td>\n",
       "      <td>NEC</td>\n",
       "      <td>30</td>\n",
       "      <td>92.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>43.5</td>\n",
       "      <td>45.29</td>\n",
       "      <td>64</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>32.3000</td>\n",
       "      <td>1.7333</td>\n",
       "      <td>3.4667</td>\n",
       "      <td>5.2000</td>\n",
       "      <td>1.7333</td>\n",
       "      <td>1.2333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>11.8667</td>\n",
       "      <td>10.445487</td>\n",
       "      <td>-8.5070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       player_name           team  conf  GP   Ortg   usg   eFG  TS_per  FTM  \\\n",
       "0    DeAndrae Ross  South Alabama    SB  26   97.3  16.6  42.5   44.43   10   \n",
       "1    Pooh Williams       Utah St.   WAC  34  108.3  14.9  52.4   54.48   30   \n",
       "2    Jesus Verdejo  South Florida    BE  27   96.2  21.8  45.7   47.98   45   \n",
       "3  Mike Hornbuckle     Pepperdine   WCC  30   97.7  16.0  53.6   53.69   14   \n",
       "4    Anthony Brown        Pacific    BW  33   96.5  22.0  52.8   54.31   64   \n",
       "5     Nick Rodgers         Butler  Horz   6    0.0   0.0   0.0    0.00    0   \n",
       "6       Dana Smith       Longwood   ind  27  104.8  23.0  53.4   56.30   94   \n",
       "7        Matt Beck        Fordham   A10  19  131.9   3.3  90.0   90.00    0   \n",
       "8  Justin Drummond         Wagner   NEC  30   99.7  20.5  48.8   53.07   76   \n",
       "9      Jamal Smith         Wagner   NEC  30   92.5  23.0  43.5   45.29   64   \n",
       "\n",
       "   FTA  ...       mp    oreb    dreb    treb     ast     stl     blk      pts  \\\n",
       "0   14  ...  14.5769  0.1923  0.6154  0.8077  1.1923  0.3462  0.0385   3.8846   \n",
       "1   45  ...  24.5294  0.6765  1.2647  1.9412  1.8235  0.4118  0.2353   5.9412   \n",
       "2   67  ...  33.1852  0.6296  2.3333  2.9630  1.9630  0.4815  0.0000  12.1852   \n",
       "3   27  ...  17.9667  0.7000  1.4333  2.1333  1.1000  0.5667  0.1333   4.9333   \n",
       "4  114  ...  22.9091  1.4242  3.3030  4.7273  0.8485  0.4545  0.3333   7.5758   \n",
       "5    0  ...   1.5000  0.0000  0.3333  0.3333  0.3333  0.0000  0.0000   0.0000   \n",
       "6  143  ...  31.1111  2.1481  3.8889  6.0370  1.8148  1.7778  0.7407  15.2593   \n",
       "7    0  ...   6.1579  0.1579  0.1579  0.3158  0.1053  0.1053  0.0000   0.4737   \n",
       "8  109  ...  33.2667  0.8000  3.9333  4.7333  4.1333  1.7333  0.8000  10.5000   \n",
       "9  120  ...  32.3000  1.7333  3.4667  5.2000  1.7333  1.2333  0.1000  11.8667   \n",
       "\n",
       "         PER   netRtg  \n",
       "0   4.603199 -11.0210  \n",
       "1   7.120085   3.2220  \n",
       "2   8.264053 -11.3560  \n",
       "3   8.035690 -11.1580  \n",
       "4   9.470778  -4.5520  \n",
       "5        NaN -92.7329  \n",
       "6  17.610147   3.5910  \n",
       "7   0.930489  13.3960  \n",
       "8  13.165177   2.1018  \n",
       "9  10.445487  -8.5070  \n",
       "\n",
       "[10 rows x 39 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#converting column types to appropriate types and dropping unneeded columns  \n",
    "data = data.drop(columns = ['Unnamed: 64', 'Unnamed: 65', 'rimmade', 'rimmade+rimmiss', 'midmade', 'midmade+midmiss'])\n",
    "data = data.drop(columns = ['num', 'Rec Rank', 'rimmade/(rimmade+rimmiss)', 'midmade/(midmade+midmiss)', 'dunksmade', 'dunksmiss+dunksmade', 'dunksmade/(dunksmade+dunksmiss)'])\n",
    "data = data.drop(columns = [])\n",
    "data = data.convert_dtypes()\n",
    "data = data.drop(columns = ['type', 'pid', 'adjoe', 'porpag', 'ht', 'Min_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'adrtg', 'dporpag', 'stops', 'gbpm', 'ogbpm', 'dgbpm'])\n",
    "data['pts'] = data['pts'].astype('float64')\n",
    "data['bpm'] = data['bpm'].astype('float64')\n",
    "data['Ortg'] = data['Ortg'].astype('float64')\n",
    "data['GP'] = data['GP'].astype('int64')\n",
    "data['FTM'] = data['FTM'].astype('int64')\n",
    "data['FTA'] = data['FTA'].astype('int64')\n",
    "data['twoPM'] = data['twoPM'].astype('int64')\n",
    "data['twoPA'] = data['twoPA'].astype('int64')\n",
    "data['TPM'] = data['TPM'].astype('int64')\n",
    "data['TPA'] = data['TPA'].astype('int64')\n",
    "data['year'] = data['year'].astype('int64')\n",
    "data['usg'] = data['usg'].astype('float64')\n",
    "data['eFG'] = data['eFG'].astype('float64')\n",
    "data['TS_per'] = data['TS_per'].astype('float64')\n",
    "data['FT_per'] = data['FT_per'].astype('float64')\n",
    "data['twoP_per'] = data['twoP_per'].astype('float64')\n",
    "data['TP_per'] = data['TP_per'].astype('float64')\n",
    "data['blk_per'] = data['blk_per'].astype('float64')\n",
    "data['stl_per'] = data['stl_per'].astype('float64')\n",
    "data['ftr'] = data['ftr'].astype('float64')\n",
    "data['pfr'] = data['pfr'].astype('float64')\n",
    "data['ast/tov'] = data['ast/tov'].astype('float64')\n",
    "data['drtg'] = data['drtg'].astype('float64')\n",
    "data['obpm'] = data['obpm'].astype('float64')\n",
    "data['dbpm'] = data['dbpm'].astype('float64')\n",
    "data['mp'] = data['mp'].astype('float64')\n",
    "data['oreb'] = data['oreb'].astype('float64')\n",
    "data['dreb'] = data['dreb'].astype('float64')\n",
    "data['treb'] = data['treb'].astype('float64')\n",
    "data['ast'] = data['ast'].astype('float64')\n",
    "data['stl'] = data['stl'].astype('float64')\n",
    "data['blk'] = data['blk'].astype('float64')\n",
    "\n",
    "# calculating PER and netRTG then adding columns to dataset\n",
    "# these stats might be usefull for classification later on\n",
    "\n",
    "PER = []\n",
    "netRtg = []\n",
    "for i in range(len(data)):\n",
    "    if data.loc[i, 'ast/tov'] == 0 or data.loc[i, 'mp'] == 0:\n",
    "        PER.append(0)\n",
    "    else:\n",
    "        PER.append((((data.loc[i, 'twoPM'] + data.loc[i, 'TPM']) * 85.910)\n",
    "                   +((data.loc[i, 'GP'] * data.loc[i, 'stl']) * 53.897)\n",
    "                   +(data.loc[i, 'TPM'] * 51.757)\n",
    "                   +(data.loc[i, 'FTM'] * 46.845)\n",
    "                   +((data.loc[i, 'GP'] * data.loc[i, 'blk']) * 39.190)\n",
    "                   +((data.loc[i, 'GP'] * data.loc[i, 'oreb']) * 39.190)\n",
    "                   +((data.loc[i, 'GP'] * data.loc[i, 'ast']) * 34.677)\n",
    "                   +((data.loc[i, 'GP'] * data.loc[i, 'dreb']) * 14.707)\n",
    "                    -((data.loc[i, 'GP'] * data.loc[i, 'pfr']) * 17.174)\n",
    "                    -((data.loc[i, 'FTA'] - data.loc[i, 'FTM']) * 20.091)\n",
    "                    -(((data.loc[i, 'twoPA'] + data.loc[i, 'TPA']) - (data.loc[i, 'twoPM'] + data.loc[i, 'TPM'])) * 39.190)\n",
    "                    -(((data.loc[i, 'ast']) / (data.loc[i, 'ast/tov']) * (data.loc[i, 'GP'])) * 53.897))\n",
    "                    * (1 / (data.loc[i, 'mp'] * data.loc[i, 'GP'])))\n",
    "\n",
    "    netRtg.append(data.loc[i, 'Ortg'] - data.loc[i, 'drtg'])\n",
    "\n",
    "\n",
    "# https://bleacherreport.com/articles/113144-cracking-the-code-how-to-calculate-hollingers-per-without-all-the-mess\n",
    "data['PER'] = PER\n",
    "data['netRtg'] = netRtg\n",
    "data['PER'] = data['PER'].astype('float64')\n",
    "data['netRtg'] = data['netRtg'].astype('float64')\n",
    "\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deec415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding pick as either 1 or 0 if they were drafted or not \n",
    "data.loc[data['pick'].notnull(), 'pick'] = 1\n",
    "data.loc[data['pick'].isna(), 'pick'] = 0\n",
    "\n",
    "data['pick'] = data['pick'].astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94aefabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61061 entries, 0 to 61060\n",
      "Data columns (total 39 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   player_name  61061 non-null  string \n",
      " 1   team         61061 non-null  string \n",
      " 2   conf         61061 non-null  string \n",
      " 3   GP           61061 non-null  int64  \n",
      " 4   Ortg         61061 non-null  float64\n",
      " 5   usg          61061 non-null  float64\n",
      " 6   eFG          61061 non-null  float64\n",
      " 7   TS_per       61061 non-null  float64\n",
      " 8   FTM          61061 non-null  int64  \n",
      " 9   FTA          61061 non-null  int64  \n",
      " 10  FT_per       61061 non-null  float64\n",
      " 11  twoPM        61061 non-null  int64  \n",
      " 12  twoPA        61061 non-null  int64  \n",
      " 13  twoP_per     61061 non-null  float64\n",
      " 14  TPM          61061 non-null  int64  \n",
      " 15  TPA          61061 non-null  int64  \n",
      " 16  TP_per       61061 non-null  float64\n",
      " 17  blk_per      61061 non-null  float64\n",
      " 18  stl_per      61061 non-null  float64\n",
      " 19  ftr          61061 non-null  float64\n",
      " 20  yr           60787 non-null  string \n",
      " 21  pfr          61061 non-null  float64\n",
      " 22  year         61061 non-null  int64  \n",
      " 23  ast/tov      56334 non-null  float64\n",
      " 24  pick         61061 non-null  int64  \n",
      " 25  drtg         61016 non-null  float64\n",
      " 26  bpm          61016 non-null  float64\n",
      " 27  obpm         61016 non-null  float64\n",
      " 28  dbpm         61016 non-null  float64\n",
      " 29  mp           61023 non-null  float64\n",
      " 30  oreb         61023 non-null  float64\n",
      " 31  dreb         61023 non-null  float64\n",
      " 32  treb         61023 non-null  float64\n",
      " 33  ast          61023 non-null  float64\n",
      " 34  stl          61023 non-null  float64\n",
      " 35  blk          61023 non-null  float64\n",
      " 36  pts          61023 non-null  float64\n",
      " 37  PER          56346 non-null  float64\n",
      " 38  netRtg       61016 non-null  float64\n",
      "dtypes: float64(26), int64(9), string(4)\n",
      "memory usage: 18.2 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dbb7d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_name       0\n",
      "team              0\n",
      "conf              0\n",
      "GP                0\n",
      "Ortg              0\n",
      "usg               0\n",
      "eFG               0\n",
      "TS_per            0\n",
      "FTM               0\n",
      "FTA               0\n",
      "FT_per            0\n",
      "twoPM             0\n",
      "twoPA             0\n",
      "twoP_per          0\n",
      "TPM               0\n",
      "TPA               0\n",
      "TP_per            0\n",
      "blk_per           0\n",
      "stl_per           0\n",
      "ftr               0\n",
      "yr              274\n",
      "pfr               0\n",
      "year              0\n",
      "ast/tov        4727\n",
      "pick              0\n",
      "drtg             45\n",
      "bpm              45\n",
      "obpm             45\n",
      "dbpm             45\n",
      "mp               38\n",
      "oreb             38\n",
      "dreb             38\n",
      "treb             38\n",
      "ast              38\n",
      "stl              38\n",
      "blk              38\n",
      "pts              38\n",
      "PER            4715\n",
      "netRtg           45\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking null values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82a1ec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61061 entries, 0 to 61060\n",
      "Data columns (total 39 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   player_name  61061 non-null  string \n",
      " 1   team         61061 non-null  string \n",
      " 2   conf         61061 non-null  string \n",
      " 3   GP           61061 non-null  int64  \n",
      " 4   Ortg         61061 non-null  float64\n",
      " 5   usg          61061 non-null  float64\n",
      " 6   eFG          61061 non-null  float64\n",
      " 7   TS_per       61061 non-null  float64\n",
      " 8   FTM          61061 non-null  int64  \n",
      " 9   FTA          61061 non-null  int64  \n",
      " 10  FT_per       61061 non-null  float64\n",
      " 11  twoPM        61061 non-null  int64  \n",
      " 12  twoPA        61061 non-null  int64  \n",
      " 13  twoP_per     61061 non-null  float64\n",
      " 14  TPM          61061 non-null  int64  \n",
      " 15  TPA          61061 non-null  int64  \n",
      " 16  TP_per       61061 non-null  float64\n",
      " 17  blk_per      61061 non-null  float64\n",
      " 18  stl_per      61061 non-null  float64\n",
      " 19  ftr          61061 non-null  float64\n",
      " 20  yr           60787 non-null  string \n",
      " 21  pfr          61061 non-null  float64\n",
      " 22  year         61061 non-null  int64  \n",
      " 23  ast/tov      56334 non-null  float64\n",
      " 24  pick         61061 non-null  int64  \n",
      " 25  drtg         61016 non-null  float64\n",
      " 26  bpm          61016 non-null  float64\n",
      " 27  obpm         61016 non-null  float64\n",
      " 28  dbpm         61016 non-null  float64\n",
      " 29  mp           61023 non-null  float64\n",
      " 30  oreb         61023 non-null  float64\n",
      " 31  dreb         61023 non-null  float64\n",
      " 32  treb         61023 non-null  float64\n",
      " 33  ast          61023 non-null  float64\n",
      " 34  stl          61023 non-null  float64\n",
      " 35  blk          61023 non-null  float64\n",
      " 36  pts          61023 non-null  float64\n",
      " 37  PER          56346 non-null  float64\n",
      " 38  netRtg       61016 non-null  float64\n",
      "dtypes: float64(26), int64(9), string(4)\n",
      "memory usage: 18.2 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cb5002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 56222 entries, 0 to 61057\n",
      "Data columns (total 39 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   player_name  56222 non-null  string \n",
      " 1   team         56222 non-null  string \n",
      " 2   conf         56222 non-null  string \n",
      " 3   GP           56222 non-null  int64  \n",
      " 4   Ortg         56222 non-null  float64\n",
      " 5   usg          56222 non-null  float64\n",
      " 6   eFG          56222 non-null  float64\n",
      " 7   TS_per       56222 non-null  float64\n",
      " 8   FTM          56222 non-null  int64  \n",
      " 9   FTA          56222 non-null  int64  \n",
      " 10  FT_per       56222 non-null  float64\n",
      " 11  twoPM        56222 non-null  int64  \n",
      " 12  twoPA        56222 non-null  int64  \n",
      " 13  twoP_per     56222 non-null  float64\n",
      " 14  TPM          56222 non-null  int64  \n",
      " 15  TPA          56222 non-null  int64  \n",
      " 16  TP_per       56222 non-null  float64\n",
      " 17  blk_per      56222 non-null  float64\n",
      " 18  stl_per      56222 non-null  float64\n",
      " 19  ftr          56222 non-null  float64\n",
      " 20  yr           56222 non-null  string \n",
      " 21  pfr          56222 non-null  float64\n",
      " 22  year         56222 non-null  int64  \n",
      " 23  ast/tov      56222 non-null  float64\n",
      " 24  pick         56222 non-null  int64  \n",
      " 25  drtg         56222 non-null  float64\n",
      " 26  bpm          56222 non-null  float64\n",
      " 27  obpm         56222 non-null  float64\n",
      " 28  dbpm         56222 non-null  float64\n",
      " 29  mp           56222 non-null  float64\n",
      " 30  oreb         56222 non-null  float64\n",
      " 31  dreb         56222 non-null  float64\n",
      " 32  treb         56222 non-null  float64\n",
      " 33  ast          56222 non-null  float64\n",
      " 34  stl          56222 non-null  float64\n",
      " 35  blk          56222 non-null  float64\n",
      " 36  pts          56222 non-null  float64\n",
      " 37  PER          56222 non-null  float64\n",
      " 38  netRtg       56222 non-null  float64\n",
      "dtypes: float64(26), int64(9), string(4)\n",
      "memory usage: 17.2 MB\n"
     ]
    }
   ],
   "source": [
    "#dropping all the players which we didn't have all the data for \n",
    "data = data.dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b777bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicate players by keeping highest year \n",
    "\n",
    "#sorting rows by year\n",
    "data = data.sort_values(by=['year'], ascending=False)\n",
    "#removing duplicates by keeping only first year \n",
    "data = data.drop_duplicates(subset=['player_name', 'pick','team'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c797524f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28164 entries, 61057 to 0\n",
      "Data columns (total 39 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   player_name  28164 non-null  string \n",
      " 1   team         28164 non-null  string \n",
      " 2   conf         28164 non-null  string \n",
      " 3   GP           28164 non-null  int64  \n",
      " 4   Ortg         28164 non-null  float64\n",
      " 5   usg          28164 non-null  float64\n",
      " 6   eFG          28164 non-null  float64\n",
      " 7   TS_per       28164 non-null  float64\n",
      " 8   FTM          28164 non-null  int64  \n",
      " 9   FTA          28164 non-null  int64  \n",
      " 10  FT_per       28164 non-null  float64\n",
      " 11  twoPM        28164 non-null  int64  \n",
      " 12  twoPA        28164 non-null  int64  \n",
      " 13  twoP_per     28164 non-null  float64\n",
      " 14  TPM          28164 non-null  int64  \n",
      " 15  TPA          28164 non-null  int64  \n",
      " 16  TP_per       28164 non-null  float64\n",
      " 17  blk_per      28164 non-null  float64\n",
      " 18  stl_per      28164 non-null  float64\n",
      " 19  ftr          28164 non-null  float64\n",
      " 20  yr           28164 non-null  string \n",
      " 21  pfr          28164 non-null  float64\n",
      " 22  year         28164 non-null  int64  \n",
      " 23  ast/tov      28164 non-null  float64\n",
      " 24  pick         28164 non-null  int64  \n",
      " 25  drtg         28164 non-null  float64\n",
      " 26  bpm          28164 non-null  float64\n",
      " 27  obpm         28164 non-null  float64\n",
      " 28  dbpm         28164 non-null  float64\n",
      " 29  mp           28164 non-null  float64\n",
      " 30  oreb         28164 non-null  float64\n",
      " 31  dreb         28164 non-null  float64\n",
      " 32  treb         28164 non-null  float64\n",
      " 33  ast          28164 non-null  float64\n",
      " 34  stl          28164 non-null  float64\n",
      " 35  blk          28164 non-null  float64\n",
      " 36  pts          28164 non-null  float64\n",
      " 37  PER          28164 non-null  float64\n",
      " 38  netRtg       28164 non-null  float64\n",
      "dtypes: float64(26), int64(9), string(4)\n",
      "memory usage: 8.6 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53c35cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6725.000000\n",
       "mean     2020.659182\n",
       "std         0.474019\n",
       "min      2020.000000\n",
       "25%      2020.000000\n",
       "50%      2021.000000\n",
       "75%      2021.000000\n",
       "max      2021.000000\n",
       "Name: year, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting our data by taking the year 2020 and 2021 to use for testing later. \n",
    "data[data['year'] >= 2020]['year'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9529c9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21439.000000\n",
       "mean      2014.170390\n",
       "std          3.174545\n",
       "min       2009.000000\n",
       "25%       2011.000000\n",
       "50%       2014.000000\n",
       "75%       2017.000000\n",
       "max       2019.000000\n",
       "Name: year, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['year'] < 2020]['year'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb3a80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splittnig the data by creating a df with the rows for 2020 and 2021 to be used for testing models later\n",
    "data_Testing = data[data['year'] >= 2020]\n",
    "data = data[data['year'] < 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b041194d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GP</th>\n",
       "      <th>Ortg</th>\n",
       "      <th>usg</th>\n",
       "      <th>eFG</th>\n",
       "      <th>TS_per</th>\n",
       "      <th>FTM</th>\n",
       "      <th>FTA</th>\n",
       "      <th>FT_per</th>\n",
       "      <th>twoPM</th>\n",
       "      <th>twoPA</th>\n",
       "      <th>...</th>\n",
       "      <th>mp</th>\n",
       "      <th>oreb</th>\n",
       "      <th>dreb</th>\n",
       "      <th>treb</th>\n",
       "      <th>ast</th>\n",
       "      <th>stl</th>\n",
       "      <th>blk</th>\n",
       "      <th>pts</th>\n",
       "      <th>PER</th>\n",
       "      <th>netRtg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "      <td>6725.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19.004461</td>\n",
       "      <td>92.577056</td>\n",
       "      <td>18.763257</td>\n",
       "      <td>46.079033</td>\n",
       "      <td>49.057013</td>\n",
       "      <td>26.038810</td>\n",
       "      <td>36.497398</td>\n",
       "      <td>0.606192</td>\n",
       "      <td>35.302900</td>\n",
       "      <td>71.154647</td>\n",
       "      <td>...</td>\n",
       "      <td>18.089361</td>\n",
       "      <td>0.761981</td>\n",
       "      <td>2.156309</td>\n",
       "      <td>2.918284</td>\n",
       "      <td>1.170057</td>\n",
       "      <td>0.579852</td>\n",
       "      <td>0.287951</td>\n",
       "      <td>6.366801</td>\n",
       "      <td>4.935055</td>\n",
       "      <td>-9.756488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.403845</td>\n",
       "      <td>25.371608</td>\n",
       "      <td>5.743559</td>\n",
       "      <td>16.722961</td>\n",
       "      <td>15.684702</td>\n",
       "      <td>29.219647</td>\n",
       "      <td>38.671794</td>\n",
       "      <td>0.265963</td>\n",
       "      <td>37.434297</td>\n",
       "      <td>71.608555</td>\n",
       "      <td>...</td>\n",
       "      <td>10.123592</td>\n",
       "      <td>0.702263</td>\n",
       "      <td>1.544626</td>\n",
       "      <td>2.098640</td>\n",
       "      <td>1.174147</td>\n",
       "      <td>0.474079</td>\n",
       "      <td>0.399424</td>\n",
       "      <td>4.954784</td>\n",
       "      <td>13.548377</td>\n",
       "      <td>27.133036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-345.736500</td>\n",
       "      <td>-121.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>84.700000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>41.300000</td>\n",
       "      <td>44.760000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.947400</td>\n",
       "      <td>1.294100</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>2.181800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-19.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>97.400000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>51.410000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.555600</td>\n",
       "      <td>1.880000</td>\n",
       "      <td>2.517200</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>7.219260</td>\n",
       "      <td>-5.376200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>106.800000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>54.100000</td>\n",
       "      <td>56.780000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1.076900</td>\n",
       "      <td>3.076900</td>\n",
       "      <td>4.095200</td>\n",
       "      <td>1.666700</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>9.666700</td>\n",
       "      <td>11.932740</td>\n",
       "      <td>6.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>48.200000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>282.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>472.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38.800000</td>\n",
       "      <td>5.148100</td>\n",
       "      <td>10.681800</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.903200</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>27.724100</td>\n",
       "      <td>66.523333</td>\n",
       "      <td>215.485500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                GP         Ortg          usg          eFG       TS_per  \\\n",
       "count  6725.000000  6725.000000  6725.000000  6725.000000  6725.000000   \n",
       "mean     19.004461    92.577056    18.763257    46.079033    49.057013   \n",
       "std       8.403845    25.371608     5.743559    16.722961    15.684702   \n",
       "min       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%      13.000000    84.700000    15.000000    41.300000    44.760000   \n",
       "50%      20.000000    97.400000    18.400000    48.400000    51.410000   \n",
       "75%      26.000000   106.800000    22.200000    54.100000    56.780000   \n",
       "max      34.000000   300.000000    48.200000   150.000000   150.000000   \n",
       "\n",
       "               FTM          FTA       FT_per        twoPM        twoPA  ...  \\\n",
       "count  6725.000000  6725.000000  6725.000000  6725.000000  6725.000000  ...   \n",
       "mean     26.038810    36.497398     0.606192    35.302900    71.154647  ...   \n",
       "std      29.219647    38.671794     0.265963    37.434297    71.608555  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       4.000000     6.000000     0.500000     6.000000    14.000000  ...   \n",
       "50%      16.000000    24.000000     0.678000    23.000000    49.000000  ...   \n",
       "75%      39.000000    54.000000     0.778000    54.000000   110.000000  ...   \n",
       "max     233.000000   282.000000     1.000000   263.000000   472.000000  ...   \n",
       "\n",
       "                mp         oreb         dreb         treb          ast  \\\n",
       "count  6725.000000  6725.000000  6725.000000  6725.000000  6725.000000   \n",
       "mean     18.089361     0.761981     2.156309     2.918284     1.170057   \n",
       "std      10.123592     0.702263     1.544626     2.098640     1.174147   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       9.000000     0.250000     0.947400     1.294100     0.318200   \n",
       "50%      18.200000     0.555600     1.880000     2.517200     0.800000   \n",
       "75%      27.000000     1.076900     3.076900     4.095200     1.666700   \n",
       "max      38.800000     5.148100    10.681800    15.000000     8.400000   \n",
       "\n",
       "               stl          blk          pts          PER       netRtg  \n",
       "count  6725.000000  6725.000000  6725.000000  6725.000000  6725.000000  \n",
       "mean      0.579852     0.287951     6.366801     4.935055    -9.756488  \n",
       "std       0.474079     0.399424     4.954784    13.548377    27.133036  \n",
       "min       0.000000     0.000000     0.000000  -345.736500  -121.272000  \n",
       "25%       0.200000     0.038500     2.181800     0.000000   -19.468000  \n",
       "50%       0.500000     0.153800     5.250000     7.219260    -5.376200  \n",
       "75%       0.857100     0.375000     9.666700    11.932740     6.160000  \n",
       "max       3.903200     3.687500    27.724100    66.523333   215.485500  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Testing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4983fe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GP</th>\n",
       "      <th>Ortg</th>\n",
       "      <th>usg</th>\n",
       "      <th>eFG</th>\n",
       "      <th>TS_per</th>\n",
       "      <th>FTM</th>\n",
       "      <th>FTA</th>\n",
       "      <th>FT_per</th>\n",
       "      <th>twoPM</th>\n",
       "      <th>twoPA</th>\n",
       "      <th>...</th>\n",
       "      <th>mp</th>\n",
       "      <th>oreb</th>\n",
       "      <th>dreb</th>\n",
       "      <th>treb</th>\n",
       "      <th>ast</th>\n",
       "      <th>stl</th>\n",
       "      <th>blk</th>\n",
       "      <th>pts</th>\n",
       "      <th>PER</th>\n",
       "      <th>netRtg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "      <td>21439.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.578758</td>\n",
       "      <td>91.736494</td>\n",
       "      <td>18.938892</td>\n",
       "      <td>45.097826</td>\n",
       "      <td>48.309076</td>\n",
       "      <td>35.934045</td>\n",
       "      <td>51.005271</td>\n",
       "      <td>0.598905</td>\n",
       "      <td>44.399692</td>\n",
       "      <td>91.411633</td>\n",
       "      <td>...</td>\n",
       "      <td>17.800594</td>\n",
       "      <td>0.808631</td>\n",
       "      <td>2.002675</td>\n",
       "      <td>2.811306</td>\n",
       "      <td>1.141037</td>\n",
       "      <td>0.558178</td>\n",
       "      <td>0.294099</td>\n",
       "      <td>6.229846</td>\n",
       "      <td>3.651093</td>\n",
       "      <td>-11.838543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.608996</td>\n",
       "      <td>24.701687</td>\n",
       "      <td>5.784892</td>\n",
       "      <td>16.231369</td>\n",
       "      <td>15.211762</td>\n",
       "      <td>40.625832</td>\n",
       "      <td>54.124612</td>\n",
       "      <td>0.253812</td>\n",
       "      <td>47.190543</td>\n",
       "      <td>91.369662</td>\n",
       "      <td>...</td>\n",
       "      <td>10.729054</td>\n",
       "      <td>0.764652</td>\n",
       "      <td>1.552380</td>\n",
       "      <td>2.197785</td>\n",
       "      <td>1.215454</td>\n",
       "      <td>0.484217</td>\n",
       "      <td>0.438561</td>\n",
       "      <td>5.312794</td>\n",
       "      <td>16.185972</td>\n",
       "      <td>28.131672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-625.529000</td>\n",
       "      <td>-129.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>83.400000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>40.300000</td>\n",
       "      <td>44.080000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.875000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.071400</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>1.666700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-22.314400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>96.700000</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>47.600000</td>\n",
       "      <td>50.850000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.483900</td>\n",
       "      <td>0.566700</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>2.344800</td>\n",
       "      <td>0.735300</td>\n",
       "      <td>0.444400</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>4.727300</td>\n",
       "      <td>6.278512</td>\n",
       "      <td>-7.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>106.800000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>53.200000</td>\n",
       "      <td>56.050000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27.593200</td>\n",
       "      <td>1.161300</td>\n",
       "      <td>2.909100</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.621600</td>\n",
       "      <td>0.827600</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>9.921950</td>\n",
       "      <td>11.753943</td>\n",
       "      <td>4.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>607.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>5.769200</td>\n",
       "      <td>11.545500</td>\n",
       "      <td>14.533300</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>5.258100</td>\n",
       "      <td>30.090900</td>\n",
       "      <td>79.257000</td>\n",
       "      <td>1422.760000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 GP          Ortg           usg           eFG        TS_per  \\\n",
       "count  21439.000000  21439.000000  21439.000000  21439.000000  21439.000000   \n",
       "mean      23.578758     91.736494     18.938892     45.097826     48.309076   \n",
       "std        9.608996     24.701687      5.784892     16.231369     15.211762   \n",
       "min        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       17.000000     83.400000     15.000000     40.300000     44.080000   \n",
       "50%       27.000000     96.700000     18.600000     47.600000     50.850000   \n",
       "75%       31.000000    106.800000     22.500000     53.200000     56.050000   \n",
       "max       41.000000    300.000000     50.000000    150.000000    150.000000   \n",
       "\n",
       "                FTM           FTA        FT_per         twoPM         twoPA  \\\n",
       "count  21439.000000  21439.000000  21439.000000  21439.000000  21439.000000   \n",
       "mean      35.934045     51.005271      0.598905     44.399692     91.411633   \n",
       "std       40.625832     54.124612      0.253812     47.190543     91.369662   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        4.000000      8.000000      0.500000      6.000000     14.000000   \n",
       "50%       21.000000     32.000000      0.667000     28.000000     61.000000   \n",
       "75%       55.000000     79.000000      0.762000     70.000000    146.000000   \n",
       "max      312.000000    374.000000      1.000000    311.000000    607.000000   \n",
       "\n",
       "       ...            mp          oreb          dreb          treb  \\\n",
       "count  ...  21439.000000  21439.000000  21439.000000  21439.000000   \n",
       "mean   ...     17.800594      0.808631      2.002675      2.811306   \n",
       "std    ...     10.729054      0.764652      1.552380      2.197785   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      7.875000      0.250000      0.750000      1.071400   \n",
       "50%    ...     17.483900      0.566700      1.700000      2.344800   \n",
       "75%    ...     27.593200      1.161300      2.909100      4.000000   \n",
       "max    ...     50.000000      5.769200     11.545500     14.533300   \n",
       "\n",
       "                ast           stl           blk           pts           PER  \\\n",
       "count  21439.000000  21439.000000  21439.000000  21439.000000  21439.000000   \n",
       "mean       1.141037      0.558178      0.294099      6.229846      3.651093   \n",
       "std        1.215454      0.484217      0.438561      5.312794     16.185972   \n",
       "min        0.000000      0.000000      0.000000      0.000000   -625.529000   \n",
       "25%        0.250000      0.178600      0.030300      1.666700      0.000000   \n",
       "50%        0.735300      0.444400      0.137900      4.727300      6.278512   \n",
       "75%        1.621600      0.827600      0.375000      9.921950     11.753943   \n",
       "max       10.000000      3.900000      5.258100     30.090900     79.257000   \n",
       "\n",
       "             netRtg  \n",
       "count  21439.000000  \n",
       "mean     -11.838543  \n",
       "std       28.131672  \n",
       "min     -129.313000  \n",
       "25%      -22.314400  \n",
       "50%       -7.350000  \n",
       "75%        4.518900  \n",
       "max     1422.760000  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc756876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21439.000000\n",
       "mean         0.026774\n",
       "std          0.161425\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          1.000000\n",
       "Name: pick, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ensuring pick was encoded properly \n",
    "data['pick'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f11aa",
   "metadata": {},
   "source": [
    "## Data Visualization and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34470f3f",
   "metadata": {},
   "source": [
    "We will now visualize the data using correlation and heatmaps to help us see patterns and select features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "482c54ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1008 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#constructing the heat map\n",
    "plt.figure(figsize = (20,14))\n",
    "sns.heatmap(data.corr(), vmax = 0.9, square = True)\n",
    "plt.title(\"Pearson Correlation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "825d53db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sorting and displaying top correlations with pick as a bar graph\n",
    "data.corr()['pick'].sort_values(ascending=False).head(20).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ad45091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pick        1.000000\n",
       "twoPM       0.326207\n",
       "twoPA       0.304705\n",
       "FTM         0.299833\n",
       "FTA         0.296927\n",
       "pts         0.274791\n",
       "dreb        0.260931\n",
       "treb        0.250890\n",
       "blk         0.224654\n",
       "mp          0.199045\n",
       "obpm        0.193091\n",
       "bpm         0.192251\n",
       "oreb        0.191381\n",
       "stl         0.190823\n",
       "ast         0.184565\n",
       "GP          0.169572\n",
       "netRtg      0.166845\n",
       "TPM         0.166201\n",
       "TPA         0.157920\n",
       "usg         0.156263\n",
       "PER         0.150729\n",
       "Ortg        0.140109\n",
       "dbpm        0.116673\n",
       "TS_per      0.101496\n",
       "eFG         0.090623\n",
       "FT_per      0.088800\n",
       "twoP_per    0.086889\n",
       "TP_per      0.073082\n",
       "ast/tov     0.057844\n",
       "stl_per     0.036210\n",
       "blk_per     0.029026\n",
       "ftr         0.011637\n",
       "year       -0.007446\n",
       "pfr        -0.036309\n",
       "drtg       -0.100524\n",
       "Name: pick, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the correlations from highest to lowest for pick\n",
    "data.corr()['pick'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0fcb6b",
   "metadata": {},
   "source": [
    "To deem important correlations between stats and metrics, we used a correlation heat map. These correlations allowed us to choose the best attributes to train our data on. However, in order to use a proper mix of metrics and stats, an even amount of both were chosen, as a result of a bar graph visualization. twoPM had the highest correlation with pick at around 0.32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2834d32",
   "metadata": {},
   "source": [
    "## Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467430a1",
   "metadata": {},
   "source": [
    "We will now build our three models using the following features: 'bpm',\"PER\", \"netRtg\", 'pts','mp','twoPM','FTM','TPM','treb','ast'. We selected these features by looking at the correlations in the heat map, and selecting at least 3 metrics for testing. The three highest metrics were bpm PER and netRtg. Furthermore, when looking a drafted player's profile, they tend to have high stats, dude to them always playing. Hence, mp is also an important feature as well. The models we will be building are Random Forest Classifier, Logistical Regression, and Support Vector Classification. We will tune the hyperparameters using GridSearchCV and display the classification report as well as use the score function to evaulate the accuracy of the model. We will also look at mean train and test score to check for overfitting.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ca9e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the data into the input 'X' and the labels 'y'\n",
    "X = data[['bpm',\"PER\", \"netRtg\", 'pts','mp','twoPM','FTM','TPM','treb','ast']]\n",
    "\n",
    "y = data['pick']\n",
    "\n",
    "#split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, #the observations\n",
    "                                                    y, #the label\n",
    "                                                    test_size=0.3,  #set aside 30% of the data as the test set\n",
    "                                                    random_state=7, #reproduce the results\n",
    "                                                    stratify=y\n",
    "                                                    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4226eb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={'n_estimators': array([ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
       "        93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
       "       106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
       "       119])},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the random forest classification \n",
    "#using gridSearchSV to tune hyper parameters \n",
    "#we will be tuning the n_estimators paramter \n",
    "\n",
    "cf = RandomForestClassifier()\n",
    "\n",
    "params = {'n_estimators' : np.arange(80, 120)}\n",
    "\n",
    "cf_grid = GridSearchCV(estimator = cf, param_grid = params, cv=3, return_train_score = True) \n",
    "cf_grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb65012b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 101}\n",
      "0.9782102349945491\n"
     ]
    }
   ],
   "source": [
    "#best parameter and score \n",
    "print(cf_grid.best_params_)\n",
    "print(cf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f19d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct a rfc using the n_estimators parameter and random_state 7 for reproducibility \n",
    "cf = RandomForestClassifier(n_estimators=104,random_state=7)\n",
    "cf.fit(X_train, y_train)\n",
    "y_pred = cf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "166760b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is : 0.9790111940298507\n"
     ]
    }
   ],
   "source": [
    "#accuracy using score function \n",
    "print('The accuracy of the model is : {}'.format(cf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "489b31a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Drafted       0.98      1.00      0.99      6260\n",
      "     Drafted       0.73      0.34      0.47       172\n",
      "\n",
      "    accuracy                           0.98      6432\n",
      "   macro avg       0.86      0.67      0.73      6432\n",
      "weighted avg       0.98      0.98      0.98      6432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report  \n",
    "print(classification_report(y_test, y_pred, target_names = [\"Not Drafted\", \"Drafted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f89807",
   "metadata": {},
   "source": [
    "The Random forest clasification model was able to classify if the player was drafted or not in the training data with a high accuracy. The accuracy score was good. The model is able to idenitfy not drafted with an accuracy of 0.98 and drafted with an accuracy of .73. This means that the model is much better at classifying correctly if a player is not drafted then if they are drafted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba493ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.978277</td>\n",
       "      <td>0.001405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978277</td>\n",
       "      <td>0.000926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.978210</td>\n",
       "      <td>0.000431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.978077</td>\n",
       "      <td>0.000338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978010</td>\n",
       "      <td>0.000862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_train_score  std_train_score  mean_test_score  std_test_score\n",
       "5           0.999967         0.000047         0.978277        0.001405\n",
       "35          1.000000         0.000000         0.978277        0.000926\n",
       "9           0.999967         0.000047         0.978210        0.000431\n",
       "39          0.999967         0.000047         0.978077        0.000338\n",
       "33          1.000000         0.000000         0.978010        0.000862"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the cv results for the train and test set\n",
    "cv_results = pd.DataFrame(cf_grid.cv_results_)\n",
    "cv_results = cv_results.sort_values('mean_test_score', ascending=False)\n",
    "cv_results[['mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500cff1",
   "metadata": {},
   "source": [
    "The highest mean train score is 1 and the highest mean test score is 0.97. There is a slight difference which could indicate that the model is slightly overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae73e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "importances = permutation_importance(cf,X_test,y_test, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14df9db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.84079602e-03,  9.01741294e-04,  1.18159204e-03,  9.01741294e-04,\n",
       "        2.17661692e-04,  3.91791045e-03,  1.15049751e-03,  1.15049751e-03,\n",
       "       -4.66417910e-04,  6.21890547e-05])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at feature importance \n",
    "importances.importances_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e03b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = importances.importances_mean\n",
    "data_columns = X.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87c9332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = np.argsort(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee366832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['treb', 'ast', 'mp', 'pts', 'PER', 'FTM', 'TPM', 'netRtg', 'twoPM',\n",
       "       'bpm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features in order of importance leaast to greatest \n",
    "data_columns[sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26238f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='feature'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#graphing the importances of the features \n",
    "d = {'importance': importance, 'feature': data_columns}\n",
    "importance_df = pd.DataFrame(data=d)\n",
    "importance_df.groupby('feature').mean().sort_values(by='importance',ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcfd8216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bpm</th>\n",
       "      <td>0.006841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twoPM</th>\n",
       "      <td>0.003918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>netRtg</th>\n",
       "      <td>0.001182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTM</th>\n",
       "      <td>0.001150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TPM</th>\n",
       "      <td>0.001150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.000902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pts</th>\n",
       "      <td>0.000902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp</th>\n",
       "      <td>0.000218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ast</th>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treb</th>\n",
       "      <td>-0.000466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         importance\n",
       "feature            \n",
       "bpm        0.006841\n",
       "twoPM      0.003918\n",
       "netRtg     0.001182\n",
       "FTM        0.001150\n",
       "TPM        0.001150\n",
       "PER        0.000902\n",
       "pts        0.000902\n",
       "mp         0.000218\n",
       "ast        0.000062\n",
       "treb      -0.000466"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importances \n",
    "importance_df.groupby('feature').mean().sort_values(by='importance',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54086c82",
   "metadata": {},
   "source": [
    "The most important features seem to bpm towpm and netRTG. The least important are Ast and Treb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3d0c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the Logistical Regression Model\n",
    "lr = LogisticRegression()\n",
    "params = {'C' : np.arange(0.1, 5,0.1)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a96748e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LogisticRegression(),\n",
       "             param_grid={'C': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3,\n",
       "       1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5, 2.6,\n",
       "       2.7, 2.8, 2.9, 3. , 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9,\n",
       "       4. , 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9])},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using gridsearch to tune C\n",
    "lr_grid = GridSearchCV(estimator = lr, param_grid = params, cv=3, return_train_score = True) \n",
    "lr_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "383d93f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3.9000000000000004}\n",
      "0.9749450742722433\n"
     ]
    }
   ],
   "source": [
    "#best C value and best score\n",
    "print(lr_grid.best_params_)\n",
    "print(lr_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "842f72d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#buidling the LR model with 3.9 C value and random_state 7 for reproducibility\n",
    "lr = LogisticRegression(C=3.9,random_state=7)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42c0d95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is : 0.9755907960199005\n"
     ]
    }
   ],
   "source": [
    "#evaluating the accuracy with score function \n",
    "print('The accuracy of the model is : {}'.format(lr.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50b7a376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Drafted       0.98      1.00      0.99      6260\n",
      "     Drafted       0.64      0.20      0.31       172\n",
      "\n",
      "    accuracy                           0.98      6432\n",
      "   macro avg       0.81      0.60      0.65      6432\n",
      "weighted avg       0.97      0.98      0.97      6432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#displaying the classification report \n",
    "print(classification_report(y_test, y_pred, target_names = [\"Not Drafted\", \"Drafted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0ad53",
   "metadata": {},
   "source": [
    "The Logistical Regression model predicts the classifaction of drafting with a high accuracy. Once again the not drafted score is higher than the drafted meaning that this model is better at classifying a player who was not drafted than it is at classifying a player that was drafted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "818c1176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.974778</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.974945</td>\n",
       "      <td>0.000571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.974878</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.974945</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.974778</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.974878</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.974245</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.974878</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.974512</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.974812</td>\n",
       "      <td>0.001177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_train_score  std_train_score  mean_test_score  std_test_score\n",
       "38          0.974778         0.000524         0.974945        0.000571\n",
       "22          0.974878         0.001064         0.974945        0.000247\n",
       "19          0.974778         0.000836         0.974878        0.000997\n",
       "2           0.974245         0.000902         0.974878        0.001062\n",
       "15          0.974512         0.000667         0.974812        0.001177"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the cv results for the train and test set\n",
    "cv_results = pd.DataFrame(lr_grid.cv_results_)\n",
    "cv_results = cv_results.sort_values('mean_test_score', ascending=False)\n",
    "cv_results[['mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da19e0a",
   "metadata": {},
   "source": [
    "The highest mean train score was 0.97 and so was the highest mean test score. There is not much of a difference between the two indicating neither overfitting or underfitting. The model is able to accuratly predict the classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0420717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buidling the SVM Classification model \n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02338aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=SVC(),\n",
       "             param_grid={'C': array([0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. , 2.2, 2.4, 2.6,\n",
       "       2.8])},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'C': np.arange(0.2, 3,0.2)}\n",
    "#hypertuning the c value using GridSearchCV\n",
    "svm_grid = GridSearchCV(estimator = svm, param_grid = params, cv=3, return_train_score = True) \n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a78dce5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 2.2}\n",
      "0.9754115143868497\n"
     ]
    }
   ],
   "source": [
    "#Best C value and score \n",
    "print(svm_grid.best_params_)\n",
    "print(svm_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "898f2820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 0.9785447761194029\n"
     ]
    }
   ],
   "source": [
    "#bulding the SVM model with C Value of 2.2 \n",
    "svm = SVC(C=2.2,random_state=7)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_Scaled = scaler.transform(X_train) #scale the training data\n",
    "\n",
    "svm.fit(X_train_Scaled, y_train) #fit the data\n",
    "X_test_Scaled = scaler.transform(X_test) # scale the test data \n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "#getting accuracy of model using score function \n",
    "print('The accuracy of the model is: {}'.format(svm.score(X_test_Scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56da3cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Drafted       0.97      1.00      0.99      6260\n",
      "     Drafted       0.00      0.00      0.00       172\n",
      "\n",
      "    accuracy                           0.97      6432\n",
      "   macro avg       0.49      0.50      0.49      6432\n",
      "weighted avg       0.95      0.97      0.96      6432\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#displaying the classifaction report \n",
    "target_names = ['Not Drafted', 'Drafted']\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99610c71",
   "metadata": {},
   "source": [
    "The model is able to predicting the overall draft classifiction with a high accuracy. However is not able to predict the if a player is drafted accuratly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61850b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.976811</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.975412</td>\n",
       "      <td>0.000587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.976844</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.975345</td>\n",
       "      <td>0.000497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.976977</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.975345</td>\n",
       "      <td>0.000338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.977111</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.975278</td>\n",
       "      <td>0.000409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.976678</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.975212</td>\n",
       "      <td>0.000488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_train_score  std_train_score  mean_test_score  std_test_score\n",
       "10          0.976811         0.000142         0.975412        0.000587\n",
       "11          0.976844         0.000170         0.975345        0.000497\n",
       "12          0.976977         0.000124         0.975345        0.000338\n",
       "13          0.977111         0.000216         0.975278        0.000409\n",
       "9           0.976678         0.000170         0.975212        0.000488"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(svm_grid.cv_results_)\n",
    "cv_results = cv_results.sort_values('mean_test_score', ascending=False)\n",
    "cv_results[['mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9357903",
   "metadata": {},
   "source": [
    "The highest mean train score and highest mean test score are both close to eachother indicating neither overfittnig or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446414af",
   "metadata": {},
   "source": [
    "## Testing our models with 2020 and 2021 data\n",
    "We also wanted to see how our models would perform on a standard draft class so we tested our model on data from the years of 2020 and 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22e617cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_Testing[['bpm',\"PER\", \"netRtg\", 'pts','mp','twoPM','FTM','TPM','treb','ast']]\n",
    "y_test = data_Testing['pick']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ecacd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the random forest classifier \n",
    "y_pred = cf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f6160fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is : 0.9872118959107806\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of the model is : {}'.format(cf.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d1fd87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Drafted       0.99      1.00      0.99      6626\n",
      "     Drafted       0.72      0.21      0.33        99\n",
      "\n",
      "    accuracy                           0.99      6725\n",
      "   macro avg       0.86      0.61      0.66      6725\n",
      "weighted avg       0.98      0.99      0.98      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = [\"Not Drafted\", \"Drafted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95286792",
   "metadata": {},
   "source": [
    "The random forest classifier model is able to accurately classify the draft for the 2020 and 2021 data. It still classifies players who got didn't get drafted more accurately than players who got drafted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a56d4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the logistical regression model \n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8aaf20f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is : 0.9846840148698884\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of the model is : {}'.format(lr.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c169672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Drafted       0.99      1.00      0.99      6626\n",
      "     Drafted       0.36      0.05      0.09        99\n",
      "\n",
      "    accuracy                           0.98      6725\n",
      "   macro avg       0.67      0.52      0.54      6725\n",
      "weighted avg       0.98      0.98      0.98      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = [\"Not Drafted\", \"Drafted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45010ca4",
   "metadata": {},
   "source": [
    "The logistical regression model is able to accurately classify the draft for the 2020 and 2021 data. It still classifies players who got didn't get drafted more accurately than players who got drafted, but the drafted category is much less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5cfdd7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the svm model \n",
    "X_test_Scaled = scaler.transform(X_test)\n",
    "y_pred = svm.predict(X_test_Scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a64d94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidyut/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is : 0.985278810408922\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of the model is : {}'.format(svm.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d2bad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Drafted       0.99      1.00      0.99      6626\n",
      "     Drafted       0.67      0.06      0.11        99\n",
      "\n",
      "    accuracy                           0.99      6725\n",
      "   macro avg       0.83      0.53      0.55      6725\n",
      "weighted avg       0.98      0.99      0.98      6725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = [\"Not Drafted\", \"Drafted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bc44d",
   "metadata": {},
   "source": [
    "The Support Vector Classification model is able to accurately classify the draft for the 2020 and 2021 data. It classifies players who got didn't get drafted more accurately than players who got drafted, but perfoms much better than it did in the training with the new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaeb1c2",
   "metadata": {},
   "source": [
    "## Results and Evaluation\n",
    "### Random Assortment Testing Results\n",
    "We trained our models using 10 features, bpm, PER, netRtg, pts, mp, twoPM, FTM, TPM, treb, ast. For our initial train test split, the Random Forest Classification model performed the best with an accuracy score of 0.98. It was able to predict the not drafted players with 0.98 precision and the drafted with 0.73 precision. Then the Logistical Regression model had an accuracy of 0.98 and a precision of 0.98 for not drafted players, but it had a lower precision of 0.64 for drafted players. Finally the SVC model had an accuracy of 0.98 and a precision of 0.97 for not drafted players, but it had a precision of 0 for drafted payers, meaning it was not accurate in classifying drafted players. Overall, all the models were very accurate, however they were also all more precise in classifying not drafted players than drafted players. This did not skew the overall accuracy of the model because there are much more players that are not drafted so that category carries more weight.\n",
    "\n",
    "### Standard Draft Class Testing Results\n",
    "We also wanted to see how our models would perform on a standard draft class so we tested our model on data from the years of 2020 and 2021. The models performed similarly in accuracy as before, but the precision for drafted and not drafted players was different. The Random Forest model’s performance did not change. The Logistical Regression model decreased in drafted player precision to 0.36 and the Support Vector model increased to 0.67 for drafted precision. The models were still able to perform well overall with the new data but there was some variation in the performance. This can be attributed to variations in the data. Since a only a small portion of players actually get drafted, the precision is more likely to be skewed by incorrect classifications. The players not drafted on the other hand are much greater in number and this the precious stays consistent. \n",
    "\n",
    "We concluded that the Random Forrest model was the best for predicting the classifications of players as it had the highest performance overall. \n",
    "\n",
    "### Evaluation\n",
    "We derived to feature importance from the model using the permutation importance function. The importance of the features are graphed below with BPM being the most important. These results demonstrate that metrics are essential for players wanting to be drafted, and it is not the pure on-court stats, but rather the unseen effect of a player on the court that accurately details whether a player is drafted or not. However, it also clear that scoring is still highly regarded among NBA teams, as baskets made are all ranked above other measures offense such assists.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
